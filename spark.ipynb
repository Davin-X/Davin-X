{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creatinfg Spark session \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "  Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "  val spark = SparkSession.builder\n",
    "    .appName(\"SparkSessionExample\")\n",
    "    .config(\"spark.master\", \"local\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.driver.memory\", \"1g\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.default.parallelism\", \"5\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .enableHiveSupport() // If you want to enable Hive support\n",
    "    .getOrCreate()\n",
    "\n",
    "  println(\"Hello world!\", spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame\n",
    "In the following, I would like to create a fake DataFrame which contains fake bought and sold stock assets.\n",
    "\n",
    "#### First approach of a creating a DataFrame— Creating DataFrame using Tuple and .toDF() function\n",
    "The first approach for creating a data frame in Spark using Scala syntax is to use the spark.implicits._.\n",
    "\n",
    "In this approach, each row of the data frame corresponds to a tuple in which we bring the name of the columns in the .toDF() function. Let us create a DataFrame with a few rows using the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1677920153771)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "\n",
      "root\n",
      " |-- Asset: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Unit: double (nullable = false)\n",
      " |-- Trade Name: string (nullable = true)\n",
      " |-- Buy Date: string (nullable = true)\n",
      " |-- Buy Price: double (nullable = false)\n",
      " |-- Sell Price: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "data: Seq[(String, String, Double, String, String, Double, Double)] = List((Stock,Plus500,0.5938,Amazon,01/11/2021,3368.0,4000.0), (Stock,Plus500,0.6,Facebook,01/11/2021,160.0,200.0), (Stock,Plus500,0.68,Amazon,01/11/2021,160.0,200.0), (Stock,eToro,1.5,Facebook,12/11/2021,180.0,250.0), (Stock,Plus500,0.065,LinkedIn,12/11/2021,80.0,140.0), (Stock,eToro,1.3,Pfeizer,01/12/2021,34.0,85.5), (Stock,Plus500,0.01,Bitcoin,01/11/2021,45000.0,48000.0), (Stock,Plus500,0.08,Sand,29/11/2021,5.4,8.9))\r\n",
       "firstApproachDF: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "  \n",
    "val data = Seq(\n",
    "    (\"Stock\", \"Plus500\", 0.5938, \"Amazon\", \"01/11/2021\", 3368.000, 4000.0),\n",
    " (\"Stock\", \"Plus500\", 0.6, \"Facebook\", \"01/11/2021\", 160.0, 200.0),\n",
    " (\"Stock\", \"Plus500\", 0.68, \"Amazon\", \"01/11/2021\", 160.0, 200.0),\n",
    " (\"Stock\", \"eToro\", 1.5, \"Facebook\", \"12/11/2021\", 180.0, 250.0),\n",
    " (\"Stock\", \"Plus500\", 0.065, \"LinkedIn\", \"12/11/2021\", 80.0, 140.0),\n",
    " (\"Stock\", \"eToro\", 1.3, \"Pfeizer\", \"01/12/2021\", 34.0, 85.5),\n",
    " (\"Stock\", \"Plus500\", 0.01, \"Bitcoin\", \"01/11/2021\", 45000.0, 48000.0),\n",
    " (\"Stock\", \"Plus500\", 0.08, \"Sand\", \"29/11/2021\", 5.4, 8.9)\n",
    "    )\n",
    "\n",
    "val firstApproachDF = data.toDF(\"Asset\", \"Platform\", \"Unit\", \"Trade Name\", \"Buy Date\", \"Buy Price\", \"Sell Price\")\n",
    "\n",
    "firstApproachDF.show()\n",
    "\n",
    "firstApproachDF.printSchema "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second approach of creating dataframe — Using createDataFrame() function\n",
    "In the second approach, we use spark.createDataFrame() to create a DataFrame. The data still contains the sequence of tuples in Scala like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+----------+-------+-------+\n",
      "|   _1|     _2|    _3|      _4|        _5|     _6|     _7|\n",
      "+-----+-------+------+--------+----------+-------+-------+\n",
      "|Stock|Plus500|0.5938|  Amazon|01/11/2021| 3368.0| 4000.0|\n",
      "|Stock|Plus500|   0.6|Facebook|01/11/2021|  160.0|  200.0|\n",
      "|Stock|Plus500|  0.68|  Amazon|01/11/2021|  160.0|  200.0|\n",
      "|Stock|  eToro|   1.5|Facebook|12/11/2021|  180.0|  250.0|\n",
      "|Stock|Plus500| 0.065|LinkedIn|12/11/2021|   80.0|  140.0|\n",
      "|Stock|  eToro|   1.3| Pfeizer|01/12/2021|   34.0|   85.5|\n",
      "|Stock|Plus500|  0.01| Bitcoin|01/11/2021|45000.0|48000.0|\n",
      "|Stock|Plus500|  0.08|    Sand|29/11/2021|    5.4|    8.9|\n",
      "+-----+-------+------+--------+----------+-------+-------+\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: double (nullable = false)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: double (nullable = false)\n",
      " |-- _7: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, String, Double, String, String, Double, Double)] = List((Stock,Plus500,0.5938,Amazon,01/11/2021,3368.0,4000.0), (Stock,Plus500,0.6,Facebook,01/11/2021,160.0,200.0), (Stock,Plus500,0.68,Amazon,01/11/2021,160.0,200.0), (Stock,eToro,1.5,Facebook,12/11/2021,180.0,250.0), (Stock,Plus500,0.065,LinkedIn,12/11/2021,80.0,140.0), (Stock,eToro,1.3,Pfeizer,01/12/2021,34.0,85.5), (Stock,Plus500,0.01,Bitcoin,01/11/2021,45000.0,48000.0), (Stock,Plus500,0.08,Sand,29/11/2021,5.4,8.9))\r\n",
       "secondApproachDF: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "    (\"Stock\", \"Plus500\", 0.5938, \"Amazon\", \"01/11/2021\", 3368.000, 4000.0),\n",
    " (\"Stock\", \"Plus500\", 0.6, \"Facebook\", \"01/11/2021\", 160.0, 200.0),\n",
    " (\"Stock\", \"Plus500\", 0.68, \"Amazon\", \"01/11/2021\", 160.0, 200.0),\n",
    " (\"Stock\", \"eToro\", 1.5, \"Facebook\", \"12/11/2021\", 180.0, 250.0),\n",
    " (\"Stock\", \"Plus500\", 0.065, \"LinkedIn\", \"12/11/2021\", 80.0, 140.0),\n",
    " (\"Stock\", \"eToro\", 1.3, \"Pfeizer\", \"01/12/2021\", 34.0, 85.5),\n",
    " (\"Stock\", \"Plus500\", 0.01, \"Bitcoin\", \"01/11/2021\", 45000.0, 48000.0),\n",
    " (\"Stock\", \"Plus500\", 0.08, \"Sand\", \"29/11/2021\", 5.4, 8.9)\n",
    "    )\n",
    "    \n",
    " val secondApproachDF =  spark.createDataFrame(data)\n",
    "   \n",
    " \n",
    " secondApproachDF.show()\n",
    " secondApproachDF.printSchema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema and columns of a DataFrame\n",
    "It is always possible to use the following methods to see the schema and columns of a DataFrame in Spark. Let us consider the firstAppachDF defined in previous sub section as follows:\n",
    "\n",
    "Schema of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.types.StructType = StructType(StructField(Asset,StringType,true),StructField(Platform,StringType,true),StructField(Unit,DoubleType,false),StructField(Trade Name,StringType,true),StructField(Buy Date,StringType,true),StructField(Buy Price,DoubleType,false),StructField(Sell Price,DoubleType,false))\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstApproachDF.schema\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing schema of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Asset: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Unit: double (nullable = false)\n",
      " |-- Trade Name: string (nullable = true)\n",
      " |-- Buy Date: string (nullable = true)\n",
      " |-- Buy Price: double (nullable = false)\n",
      " |-- Sell Price: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstApproachDF.printSchema\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[String] = Array(Asset, Platform, Unit, Trade Name, Buy Date, Buy Price, Sell Price)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstApproachDF.columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A single column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Column = Asset\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstApproachDF.col(\"Asset\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating over columns of a DataFrame using Map function and appending values\n",
    "Sometimes, it is required to go through every column of a DataFrame and append a suffix. It is possible to achieve that using a map() function like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[String] = Array(Asset_suffix, Platform_suffix, Unit_suffix, Trade Name_suffix, Buy Date_suffix, Buy Price_suffix, Sell Price_suffix)\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstApproachDF.columns.map(col => col + \"_suffix\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various approaches for selecting columns of a DataFrame\n",
    "There are various ways to select columns of a DataFrame. Selecting the columns of a DataFrame is important due to two aspects:\n",
    "\n",
    "You might see various approaches in code so it is better to see these formats.\n",
    "You may need to add some sort of dynamics to your code in order to expand a DataFrame.\n",
    "Approach 1. using select and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DF: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DF = firstApproachDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|Asset|Platform|\n",
      "+-----+--------+\n",
      "|Stock| Plus500|\n",
      "|Stock| Plus500|\n",
      "|Stock| Plus500|\n",
      "|Stock|   eToro|\n",
      "|Stock| Plus500|\n",
      "+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.select(\"Asset\",\"Platform\").show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 and 3. using select with $ sign or col( )\n",
    "I brought the second and third approaches as it is possible to use them together like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|\n",
      "+-----+--------+------+----------+----------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|\n",
      "+-----+--------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.select($\"Asset\", $\"Platform\", $\"Unit\", col(\"Trade Name\"), \n",
    "col(\"Buy Date\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 4. using expressions and spark implicits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Asset|Trade Name|\n",
      "+-----+----------+\n",
      "|Stock|    Amazon|\n",
      "|Stock|  Facebook|\n",
      "|Stock|    Amazon|\n",
      "|Stock|  Facebook|\n",
      "|Stock|  LinkedIn|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.select(expr(\"Asset\"), $\"Trade Name\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to apply calculations using expr. Let us consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+\n",
      "|  Unit|Unit Multiplied by 2|Trade Name|\n",
      "+------+--------------------+----------+\n",
      "|0.5938|              1.1876|    Amazon|\n",
      "|   0.6|                 1.2|  Facebook|\n",
      "|  0.68|                1.36|    Amazon|\n",
      "|   1.5|                 3.0|  Facebook|\n",
      "| 0.065|                0.13|  LinkedIn|\n",
      "+------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.select($\"Unit\",expr(\"Unit * 2\").as(\"Unit Multiplied by 2\"), \n",
    "$\"Trade Name\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 5. using selectExpr\n",
    "Let us consider the following example of calculating profit from the difference of Buy Price and Sell Price using selectExpr function. Please note that we should use `` for columns with spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+----------+------+------+\n",
      "|Asset|Platform|Trade Name|  Buy Date|  Unit|profit|\n",
      "+-----+--------+----------+----------+------+------+\n",
      "|Stock| Plus500|    Amazon|01/11/2021|0.5938| 632.0|\n",
      "|Stock| Plus500|  Facebook|01/11/2021|   0.6|  40.0|\n",
      "|Stock| Plus500|    Amazon|01/11/2021|  0.68|  40.0|\n",
      "|Stock|   eToro|  Facebook|12/11/2021|   1.5|  70.0|\n",
      "|Stock| Plus500|  LinkedIn|12/11/2021| 0.065|  60.0|\n",
      "+-----+--------+----------+----------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.selectExpr(\"Asset\", \"Platform\", \"\"\"`Trade Name`\"\"\", \"\"\"`Buy Date`\"\"\", \n",
    "\"Unit\", \"\"\" `Sell Price` - `Buy Price` as profit\"\"\" ).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Renaming a column and creating a new column\n",
    "It is possible to rename the column name using .withColumnRenamed() function, so let us have a look at it by adding underscore (_) to the column names with spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Asset|Platform|  Unit|Trade_Name|  Buy Date|Buy_Price|Sell_Price|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "renamedColumns: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val renamedColumns = DF.withColumnRenamed(\"Trade Name\",\"Trade_Name\")\n",
    ".withColumnRenamed(\"Buy Price\",\"Buy_Price\")\n",
    ".withColumnRenamed(\"Sell Price\",\"Sell_Price\")\n",
    "\n",
    "renamedColumns.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating a new column, we can use the function .withColumn() in which we can specify the column name as well. In the following, we create two columns named “Profit” and “Profit Percentage”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+------+------------------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|Profit| Profit Percentage|\n",
      "+-----+--------+------+----------+----------+---------+----------+------+------------------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0| 632.0| 1.187648456057007|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|  40.0|              1.25|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|  40.0|              1.25|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|  70.0|1.3888888888888888|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|  60.0|              1.75|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|  51.5| 2.514705882352941|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|3000.0|1.0666666666666667|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|   3.5|1.6481481481481481|\n",
      "+-----+--------+------+----------+----------+---------+----------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "profitDF: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 7 more fields]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val profitDF = DF.withColumn(\"Profit\", col(\"Sell Price\") - col(\"Buy Price\"))\n",
    ".withColumn(\"Profit Percentage\", col(\"Sell Price\") / col(\"Buy Price\"))\n",
    "\n",
    "profitDF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering rows in Spark\n",
    "Oftentimes, it is required to filter rows of DataFrame based on certain criteria. For doing so, it is possible to use filter function in which in the following, I review some example of filter functions:\n",
    "\n",
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.filter(col(\"Platform\") === \"Plus500\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2. using OR operator (||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orExampleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Asset: string, Platform: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orExampleDF = DF.filter(col(\"Trade Name\") === \"Amazon\" || col(\"Trade Name\") ===\"Facebook\")\n",
    "\n",
    "orExampleDF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3. using AND operator (&&)\n",
    "Keeping only Facebook for the Trade Name and Plus500 as the platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+----------+----------+---------+----------+\n",
      "|Asset|Platform|Unit|Trade Name|  Buy Date|Buy Price|Sell Price|\n",
      "+-----+--------+----+----------+----------+---------+----------+\n",
      "|Stock| Plus500| 0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "+-----+--------+----+----------+----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "andExampleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Asset: string, Platform: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val andExampleDF = DF.filter(col(\"Trade Name\") === \"Facebook\" && col(\"Platform\") ===\"Plus500\")\n",
    "\n",
    "andExampleDF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4. Not equal and greater than (>)\n",
    "Let us select all trades with more than 0.5 unit in which the trade name is not Amazon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+----------+----------+---------+----------+\n",
      "|Asset|Platform|Unit|Trade Name|  Buy Date|Buy Price|Sell Price|\n",
      "+-----+--------+----+----------+----------+---------+----------+\n",
      "|Stock| Plus500| 0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "|Stock|   eToro| 1.5|  Facebook|12/11/2021|    180.0|     250.0|\n",
      "|Stock|   eToro| 1.3|   Pfeizer|01/12/2021|     34.0|      85.5|\n",
      "+-----+--------+----+----------+----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "demoDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Asset: string, Platform: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val demoDF = DF.filter(col(\"Unit\") >= 0.5 && col(\"Trade Name\") =!= \"Amazon\")\n",
    "\n",
    "demoDF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark grouping and aggregation functions\n",
    "It is very much common to calculate several statistics over groups of data. Spark has a very rich support for such calculations. Considering the DF from previous sections, let us calculate several statistics:\n",
    "\n",
    "Example of count()\n",
    "Here, we group by per Asset and Platform and count the number of trades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------------------+\n",
      "|Asset|Platform|NumberOfTradesPerAssetPPlatForm|\n",
      "+-----+--------+-------------------------------+\n",
      "|Stock| Plus500|                              6|\n",
      "|Stock|   eToro|                              2|\n",
      "+-----+--------+-------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numberOfTradesPerPlatform: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculating number of trades per platform\n",
    "\n",
    "val numberOfTradesPerPlatform = DF.groupBy(\"Asset\", \"Platform\")\n",
    ".agg(count(\"*\").as(\"NumberOfTradesPerAssetPPlatForm\"))\n",
    "\n",
    "numberOfTradesPerPlatform.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of countDistinct()\n",
    "Here, we group by per Asset and Platform and count the distinct values of traded items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------------+\n",
      "|Asset|Platform|Number of Assets|\n",
      "+-----+--------+----------------+\n",
      "|Stock|   eToro|               2|\n",
      "|Stock| Plus500|               5|\n",
      "+-----+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numberOfAssets: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count distinct of number of traded items\n",
    "val numberOfAssets = DF.groupBy(\"Asset\",\"Platform\")\n",
    ".agg(countDistinct(\"Trade Name\").as(\"Number of Assets\"))\n",
    "\n",
    "numberOfAssets.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Sum, AVG, Median, Min, Max\n",
    "In the following, we calculate sum, avg, min, max and P50 of buy and sell prices over over Asset, Platform and Trade Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+--------------+-------------+\n",
      "|Asset|Platform|Trade Name|buy_price_sum|sell_price_Sum|buy_price_min|sell_price_min|buy_price_max|sell_price_max|buy_price_avg|sell_price_avg|sell_price_P50|buy_price_P50|\n",
      "+-----+--------+----------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+--------------+-------------+\n",
      "|Stock| Plus500|    Amazon|       3528.0|        4200.0|        160.0|         200.0|       3368.0|        4000.0|       1764.0|        2100.0|        2100.0|       1764.0|\n",
      "|Stock| Plus500|  Facebook|        160.0|         200.0|        160.0|         200.0|        160.0|         200.0|        160.0|         200.0|         200.0|        160.0|\n",
      "|Stock|   eToro|  Facebook|        180.0|         250.0|        180.0|         250.0|        180.0|         250.0|        180.0|         250.0|         250.0|        180.0|\n",
      "|Stock| Plus500|  LinkedIn|         80.0|         140.0|         80.0|         140.0|         80.0|         140.0|         80.0|         140.0|         140.0|         80.0|\n",
      "|Stock|   eToro|   Pfeizer|         34.0|          85.5|         34.0|          85.5|         34.0|          85.5|         34.0|          85.5|          85.5|         34.0|\n",
      "|Stock| Plus500|   Bitcoin|      45000.0|       48000.0|      45000.0|       48000.0|      45000.0|       48000.0|      45000.0|       48000.0|       48000.0|      45000.0|\n",
      "|Stock| Plus500|      Sand|          5.4|           8.9|          5.4|           8.9|          5.4|           8.9|          5.4|           8.9|           8.9|          5.4|\n",
      "+-----+--------+----------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "agg_df: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val agg_df = DF.groupBy(\"Asset\", \"Platform\", \"Trade Name\").agg(\n",
    "    \n",
    "    \n",
    "    sum(\"Buy Price\").as(\"buy_price_sum\"),\n",
    "    sum(\"Sell Price\").as(\"sell_price_Sum\"),\n",
    "    \n",
    "    min(\"Buy Price\").as(\"buy_price_min\"),\n",
    "    min(\"Sell Price\").as(\"sell_price_min\"),\n",
    "    \n",
    "    max(\"Buy Price\").as(\"buy_price_max\"),\n",
    "    max(\"Sell Price\").as(\"sell_price_max\"),\n",
    "    \n",
    "    avg(\"Buy Price\").as(\"buy_price_avg\"),\n",
    "    avg(\"Sell Price\").as(\"sell_price_avg\"),\n",
    "    \n",
    "    expr(\"percentile(`Sell Price`, 0.5)\").as(\"sell_price_P50\"),\n",
    "    expr(\"percentile(`Buy Price`, 0.5)\").as(\"buy_price_P50\")\n",
    "    )\n",
    "    \n",
    " agg_df.show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark dynamic groupBy and aggregation\n",
    "On many occasions, you would like to perform aggregations on a lot of columns e.g., between 10 to 100 columns, and you would like to apply different aggregation functions, so it would not be practical to write all the code so it would be more efficient to use a bit of syntactic sugar using the map function. In another tutorial, I have explained in detail the Spark dynamic groupBy aggregation. I try to explain with an example but if you need further explanation, please refer to the link above.\n",
    "\n",
    "Let us now perform the same groupBy and aggregation from the exact previous section as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Asset: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Trade Name: string (nullable = true)\n",
      " |-- Sell Price_sum: double (nullable = true)\n",
      " |-- Buy Price_sum: double (nullable = true)\n",
      " |-- Unit_sum: double (nullable = true)\n",
      " |-- Sell Price_avg: double (nullable = true)\n",
      " |-- Buy Price_avg: double (nullable = true)\n",
      " |-- Unit_avg: double (nullable = true)\n",
      " |-- Sell Price_min: double (nullable = true)\n",
      " |-- Buy Price_min: double (nullable = true)\n",
      " |-- Unit_min: double (nullable = true)\n",
      " |-- Sell Price_max: double (nullable = true)\n",
      " |-- Buy Price_max: double (nullable = true)\n",
      " |-- Unit_max: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intended_columns: List[String] = List(Sell Price, Buy Price, Unit)\r\n",
       "sumExpr: List[org.apache.spark.sql.Column] = List(sum(Sell Price) AS `Sell Price_sum`, sum(Buy Price) AS `Buy Price_sum`, sum(Unit) AS Unit_sum)\r\n",
       "avgExpr: List[org.apache.spark.sql.Column] = List(avg(Sell Price) AS `Sell Price_avg`, avg(Buy Price) AS `Buy Price_avg`, avg(Unit) AS Unit_avg)\r\n",
       "minExpr: List[org.apache.spark.sql.Column] = List(min(Sell Price) AS `Sell Price_min`, min(Buy Price) AS `Buy Price_min`, min(Unit) AS Unit_min)\r\n",
       "maxExpr: List[org.apache.spark.sql.Column] = List(max(Sell Price) AS `Sell Price_max`, max(Buy Price) AS `Buy Price_max`, max(Unit) AS Unit_max)\r\n",
       "aggExpression: List[org.apache.spark.sql.Column] = List(sum(Sell Price) AS `Sell Price_sum`, sum(Buy Price) AS `Buy Price_sum`, sum(Unit) AS Unit...\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val intended_columns = List(\"Sell Price\",\"Buy Price\", \"Unit\")\n",
    "\n",
    "val sumExpr = intended_columns.map(col => sum(col).as(col + \"_sum\"))\n",
    "val avgExpr = intended_columns.map(col => avg(col).as(col + \"_avg\"))\n",
    "val minExpr = intended_columns.map(col => min(col).as(col + \"_min\"))\n",
    "val maxExpr = intended_columns.map(col => max(col).as(col + \"_max\"))\n",
    "\n",
    "val aggExpression = sumExpr ++ avgExpr ++ minExpr ++ maxExpr\n",
    "\n",
    "val agg_df_v2 = DF.groupBy(\"Asset\", \"Platform\", \"Trade Name\")\n",
    ".agg(aggExpression.head, aggExpression.tail: _*)\n",
    "\n",
    "\n",
    "agg_df_v2.printSchema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a list of columns that are interested in performing aggregations i.e., Sell Price, Buy Price, and Unit. Now, we define the expressions. An example is sumExpr in which it iterates over all the columns in the intended_columns, sums the column value, and renames it. Afterward, we create a final aggExpression which is the concatenation of all the other expressions. We use aggExpression.head and aggExpression.tail in the agg() function together with “:_*” to consider all the expressions.\n",
    "\n",
    "The output is very huge so I cannot print all the columns, but here is the schema:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can imagine if we have a lot of columns, how much flexibility this approach can give us. We can also add the same flexibility to the list of column names in the groupBy expression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Join\n",
    "You can perform joins in two ways in which is brought by an example:\n",
    "\n",
    "We saw these two DataFrames in previous sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numberOfTradesPerPlatform: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 1 more field]\r\n",
       "numberOfAssets: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numberOfTradesPerPlatform = DF.groupBy(\"Asset\", \"Platform\")\n",
    ".agg(count(\"*\").as(\"NumberOfTradesPerAssetPPlatForm\"))\n",
    "\n",
    "\n",
    "\n",
    "val numberOfAssets = DF.groupBy(\"Asset\",\"Platform\")\n",
    ".agg(countDistinct(\"Trade Name\").as(\"Number of Assets\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach for join using Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------------------+----------------+\n",
      "|Asset|Platform|NumberOfTradesPerAssetPPlatForm|Number of Assets|\n",
      "+-----+--------+-------------------------------+----------------+\n",
      "|Stock|   eToro|                              2|               2|\n",
      "|Stock| Plus500|                              6|               5|\n",
      "+-----+--------+-------------------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinedDF: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// First approach for join\n",
    "\n",
    "val joinedDF = numberOfTradesPerPlatform\n",
    ".join(numberOfAssets, Seq(\"Asset\",\"Platform\"))\n",
    "\n",
    "joinedDF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Approach for join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------------------+-----+--------+----------------+\n",
      "|Asset|Platform|NumberOfTradesPerAssetPPlatForm|Asset|Platform|Number of Assets|\n",
      "+-----+--------+-------------------------------+-----+--------+----------------+\n",
      "|Stock|   eToro|                              2|Stock|   eToro|               2|\n",
      "|Stock| Plus500|                              6|Stock| Plus500|               5|\n",
      "+-----+--------+-------------------------------+-----+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinedDF2: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Second approach for join\n",
    "\n",
    "val joinedDF2 = numberOfTradesPerPlatform.join(numberOfAssets, \n",
    "numberOfTradesPerPlatform.col(\"Asset\") === numberOfAssets.col(\"Asset\") \n",
    "&& numberOfTradesPerPlatform.col(\"Platform\") === numberOfAssets.col(\"Platform\"))\n",
    "joinedDF2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second approach, it can be seen that some of the columns are duplicated, so it is recommended to rename them before joining. We can repeat the second approach as follows to get rid of the repetitive columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// First renaming the columns in join and creating a new DF\n",
    "\n",
    "val numberOfAssets_renamed = numberOfAssets\n",
    ".withColumnRenamed(\"Asset\",\"AssetV2\")\n",
    ".withColumnRenamed(\"Platform\",\"PlatformV2\")\n",
    "\n",
    "// performing the join with the new DF and dropping the renamed columns\n",
    "\n",
    "val joinedDF3 = numberOfTradesPerPlatform\n",
    ".join(numberOfAssets_renamed, \n",
    "numberOfTradesPerPlatform.col(\"Asset\") === numberOfAssets_renamed.col(\"AssetV2\") \n",
    "&& numberOfTradesPerPlatform.col(\"Platform\") === numberOfAssets_renamed.col(\"PlatformV2\"))\n",
    ".drop(\"AssetV2\",\"PlatformV2\")\n",
    "\n",
    "joinedDF3.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, it is possible to figure out that first the columns “Asset” and “Platform” are renamed to “AssetV2” and “PlatformV2”. Then the new DataFrame is used for the join, and finally, the renamed columns are dropped. Now, it should be possible to see the output without any duplicates:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Function — An alternative for groupBy()\n",
    "The window function allows one to perform certain calculations on specific dimensions of the data and add the result to the DataFrame. In fact, instead of performing a groupBy/aggregation and joining back the data with the original DataFrame, you still have the opportunity to perform the same by using the Window function. In case this is not clear, let us investigate it with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+-------------+--------------+--------------+-------------+--------------+-------------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|sum_buy_price|sum_sell_price|min_sell_price|min_buy_price|avg_sell_price|avg_buy_price|\n",
      "+-----+--------+------+----------+----------+---------+----------+-------------+--------------+--------------+-------------+--------------+-------------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|       3528.0|        4200.0|         200.0|        160.0|        2100.0|       1764.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|       3528.0|        4200.0|         200.0|        160.0|        2100.0|       1764.0|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|      45000.0|       48000.0|       48000.0|      45000.0|       48000.0|      45000.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|        160.0|         200.0|         200.0|        160.0|         200.0|        160.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|         80.0|         140.0|         140.0|         80.0|         140.0|         80.0|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|          5.4|           8.9|           8.9|          5.4|           8.9|          5.4|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|        180.0|         250.0|         250.0|        180.0|         250.0|        180.0|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|         34.0|          85.5|          85.5|         34.0|          85.5|         34.0|\n",
      "+-----+--------+------+----------+----------+---------+----------+-------------+--------------+--------------+-------------+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n",
       "custom_partition: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@4c2132a5\r\n",
       "resulting_df: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val custom_partition = Window.partitionBy($\"Asset\", $\"Platform\", $\"Trade Name\")\n",
    "\n",
    "val resulting_df = DF.withColumn(\"sum_buy_price\", \n",
    "sum(\"Buy Price\").over(custom_partition))\n",
    ".withColumn(\"sum_sell_price\", sum(\"Sell Price\").over(custom_partition))\n",
    ".withColumn(\"min_sell_price\", min(\"Sell Price\").over(custom_partition))\n",
    ".withColumn(\"min_buy_price\", min(\"Buy Price\").over(custom_partition))\n",
    ".withColumn(\"avg_sell_price\", avg(\"Sell Price\").over(custom_partition))\n",
    ".withColumn(\"avg_buy_price\", avg(\"Buy Price\").over(custom_partition))\n",
    "\n",
    "\n",
    "resulting_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see above that it seems like a groupBy/aggregation and joins back with the original DataFrame. Let us prove it by performing groupBy/aggregation and joining back and see if we can see the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+------+----------+---------+----------+-------------+--------------+-------------+--------------+--------------+-------------+\n",
      "|Asset|Platform|Trade Name|  Unit|  Buy Date|Buy Price|Sell Price|sum_buy_price|sum_sell_price|min_buy_price|min_sell_price|avg_sell_price|avg_buy_price|\n",
      "+-----+--------+----------+------+----------+---------+----------+-------------+--------------+-------------+--------------+--------------+-------------+\n",
      "|Stock| Plus500|    Amazon|  0.68|01/11/2021|    160.0|     200.0|       3528.0|        4200.0|        160.0|         200.0|        2100.0|       1764.0|\n",
      "|Stock| Plus500|    Amazon|0.5938|01/11/2021|   3368.0|    4000.0|       3528.0|        4200.0|        160.0|         200.0|        2100.0|       1764.0|\n",
      "|Stock| Plus500|  Facebook|   0.6|01/11/2021|    160.0|     200.0|        160.0|         200.0|        160.0|         200.0|         200.0|        160.0|\n",
      "|Stock|   eToro|  Facebook|   1.5|12/11/2021|    180.0|     250.0|        180.0|         250.0|        180.0|         250.0|         250.0|        180.0|\n",
      "|Stock| Plus500|  LinkedIn| 0.065|12/11/2021|     80.0|     140.0|         80.0|         140.0|         80.0|         140.0|         140.0|         80.0|\n",
      "|Stock|   eToro|   Pfeizer|   1.3|01/12/2021|     34.0|      85.5|         34.0|          85.5|         34.0|          85.5|          85.5|         34.0|\n",
      "|Stock| Plus500|   Bitcoin|  0.01|01/11/2021|  45000.0|   48000.0|      45000.0|       48000.0|      45000.0|       48000.0|       48000.0|      45000.0|\n",
      "|Stock| Plus500|      Sand|  0.08|29/11/2021|      5.4|       8.9|          5.4|           8.9|          5.4|           8.9|           8.9|          5.4|\n",
      "+-----+--------+----------+------+----------+---------+----------+-------------+--------------+-------------+--------------+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupBy_aggregation_df: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 7 more fields]\r\n",
       "joined_back_df: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 11 more fields]\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val groupBy_aggregation_df = DF.groupBy($\"Asset\", $\"Platform\", $\"Trade Name\")\n",
    ".agg(sum(\"Buy Price\").as(\"sum_buy_price\"), \n",
    "sum(\"Sell Price\").as(\"sum_sell_price\"), \n",
    "min(\"Buy Price\").as(\"min_buy_price\"), \n",
    "min(\"Sell Price\").as(\"min_sell_price\"), \n",
    "avg(\"Sell Price\").as(\"avg_sell_price\"),\n",
    "avg(\"Buy Price\").as(\"avg_buy_price\"))\n",
    "\n",
    "\n",
    "val joined_back_df = DF.join(groupBy_aggregation_df, \n",
    "Seq(\"Asset\", \"Platform\",\"Trade Name\"))\n",
    "\n",
    "joined_back_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark orderBy()\n",
    "Normally, after a group by and aggregation, it may be needed to see the result in a specific order of a column although this operation is very time-consuming and the DataFrame resulting from the groupBy should not be too huge. However, it is also possible to apply orderBy() on the original DataFrame. As mentioned, if the DataFrame would be huge, this operation would not be recommended. Let us apply orderBy() on the DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.orderBy(col(\"Buy Price\").desc).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to order by multiple columns and use $ instead of col() like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|\n",
      "+-----+--------+------+----------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.orderBy(col(\"Buy Price\").desc, $\"Sell Price\".desc).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank function\n",
    "Rank function together with Window allow to sort/rank the rows based on a specific column in our intended partition. Let us define a rank function by the help of Window and orderBy like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+--------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|rank_col|\n",
      "+-----+--------+------+----------+----------+---------+----------+--------+\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|       1|\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|       2|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|       3|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|       4|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|       5|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|       6|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|       1|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|       2|\n",
      "+-----+--------+------+----------+----------+---------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n",
       "custom_partition_rank: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7dbd5764\r\n",
       "resulting_df: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val custom_partition_rank = Window.partitionBy($\"Asset\", $\"Platform\")\n",
    "                            .orderBy($\"Sell Price\".desc)\n",
    "\n",
    "val resulting_df = DF.withColumn(\"rank_col\", \n",
    "                      row_number().over(custom_partition_rank))\n",
    "\n",
    "\n",
    "resulting_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that for each Platform, we can see the rank based on Sell Price in the following:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always keep the rows with your intended rank like the following in which we keep the rows with 1 or 2 rank using .filter(col(“rank_col”)<=2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+--------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|rank_col|\n",
      "+-----+--------+------+----------+----------+---------+----------+--------+\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|       1|\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|       2|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|       1|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|       2|\n",
      "+-----+--------+------+----------+----------+---------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n",
       "custom_partition_rank: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5e05b35d\r\n",
       "resulting_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Asset: string, Platform: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val custom_partition_rank = Window.partitionBy($\"Asset\", $\"Platform\")\n",
    "                            .orderBy($\"Sell Price\".desc)\n",
    "\n",
    "val resulting_df = DF.withColumn(\"rank_col\", \n",
    "                  row_number().over(custom_partition_rank))\n",
    "                  .filter(col(\"rank_col\")<=2)\n",
    "\n",
    "\n",
    "resulting_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivoting in Spark\n",
    "Pivoting in Spark is a very much useful operation and you might require to pivot your data at some point. I have written a dedicated post regarding pivoting in Spark. Below, I briefly show on our example DF an example of pivoting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------------+----------------------+-------------------+--------------------+\n",
      "|Asset|Trade Name|Plus500_sum_buy_price|Plus500_sum_sell_price|eToro_sum_buy_price|eToro_sum_sell_price|\n",
      "+-----+----------+---------------------+----------------------+-------------------+--------------------+\n",
      "|Stock|   Pfeizer|                 null|                  null|               34.0|                85.5|\n",
      "|Stock|  Facebook|                160.0|                 200.0|              180.0|               250.0|\n",
      "|Stock|   Bitcoin|              45000.0|               48000.0|               null|                null|\n",
      "|Stock|  LinkedIn|                 80.0|                 140.0|               null|                null|\n",
      "|Stock|    Amazon|               3528.0|                4200.0|               null|                null|\n",
      "|Stock|      Sand|                  5.4|                   8.9|               null|                null|\n",
      "+-----+----------+---------------------+----------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfPrivot: org.apache.spark.sql.DataFrame = [Asset: string, Trade Name: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfPrivot = DF.groupBy(\"Asset\", \"Trade Name\").pivot(\"Platform\",List(\"Plus500\", \"eToro\"))\n",
    ".agg(sum(\"Buy Price\").as(\"sum_buy_price\"), sum(\"Sell Price\").as(\"sum_sell_price\"))\n",
    "\n",
    "dfPrivot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, pivot appeared after groupBy() and before agg() functions. Also, the input to the pivot is the column name i.e., Platform, and the corresponding list of values in the column name which is Plus500 and eToro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF in Spark\n",
    "UDFs are very much helpful to write a custom function that can be applied optimised on a Spark DataFrame. For defining a UDF, we need to a. import the “org.apache.spark.sql.functions.udf” b. define our own function c. wrap our function inside the udf( _) d. and apply the UDF for creating a new column. Let us see how to use UDF in Spark with the following simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+-----------------+------------------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|buy_price_doubled|sell_price_doubled|\n",
      "+-----+--------+------+----------+----------+---------+----------+-----------------+------------------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0|           6736.0|            8000.0|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|            320.0|             400.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|            320.0|             400.0|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|            360.0|             500.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0|            160.0|             280.0|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|             68.0|             171.0|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|          90000.0|           96000.0|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|             10.8|              17.8|\n",
      "+-----+--------+------+----------+----------+---------+----------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "multiplyByTwo: (x: Float)Float\r\n",
       "multiplyByTwoUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5632/1813232473@4befa70f,FloatType,List(Some(class[value[0]: float])),Some(class[value[0]: float]),None,false,true)\r\n",
       "dfDoubled: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 7 more fields]\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Defining a simple multiplyByTwo function\n",
    "\n",
    "def multiplyByTwo(x: Float) : Float =  x*2\n",
    "\n",
    "// Wrap the function inside the udf( function_name _)\n",
    "\n",
    "val multiplyByTwoUDF = udf(multiplyByTwo _)\n",
    "\n",
    "// Apply the UDF on our intended colum names\n",
    "val dfDoubled = DF.withColumn(\"buy_price_doubled\", \n",
    "                  multiplyByTwoUDF(col(\"Buy price\")) )\n",
    "                  .withColumn(\"sell_price_doubled\", \n",
    "                    multiplyByTwoUDF(col(\"Sell Price\")))\n",
    "dfDoubled.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at a more complicated example of using multiple inputs, slightly more complicated function and using of $ sign for usage of UDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+----------+----------+---------+----------+----------+\n",
      "|Asset|Platform|  Unit|Trade Name|  Buy Date|Buy Price|Sell Price|    profit|\n",
      "+-----+--------+------+----------+----------+---------+----------+----------+\n",
      "|Stock| Plus500|0.5938|    Amazon|01/11/2021|   3368.0|    4000.0| 375.28162|\n",
      "|Stock| Plus500|   0.6|  Facebook|01/11/2021|    160.0|     200.0|      24.0|\n",
      "|Stock| Plus500|  0.68|    Amazon|01/11/2021|    160.0|     200.0|      27.2|\n",
      "|Stock|   eToro|   1.5|  Facebook|12/11/2021|    180.0|     250.0|     105.0|\n",
      "|Stock| Plus500| 0.065|  LinkedIn|12/11/2021|     80.0|     140.0| 3.8999999|\n",
      "|Stock|   eToro|   1.3|   Pfeizer|01/12/2021|     34.0|      85.5|     66.95|\n",
      "|Stock| Plus500|  0.01|   Bitcoin|01/11/2021|  45000.0|   48000.0|      30.0|\n",
      "|Stock| Plus500|  0.08|      Sand|29/11/2021|      5.4|       8.9|0.27999994|\n",
      "+-----+--------+------+----------+----------+---------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "custom_function: (unit: Float, buy: Float, sell: Float)Float\r\n",
       "custom_udf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5667/2045681611@1b8e7893,FloatType,List(Some(class[value[0]: float]), Some(class[value[0]: float]), Some(class[value[0]: float])),Some(class[value[0]: float]),None,false,true)\r\n",
       "dfUDFExample: org.apache.spark.sql.DataFrame = [Asset: string, Platform: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_function(unit: Float, buy: Float, sell: Float) : Float = {\n",
    "    \n",
    "    val diff = (sell - buy ) \n",
    "    \n",
    "    val res =  unit * diff\n",
    "    \n",
    "    res\n",
    "}\n",
    "\n",
    "val custom_udf = udf(custom_function _)\n",
    "\n",
    "val dfUDFExample = DF.withColumn(\"profit\", \n",
    "custom_udf($\"Unit\", $\"Buy Price\", $\"Sell Price\"))\n",
    "\n",
    "dfUDFExample.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has three inputs, and the corresponding UDF also accepts three parameters. So, the output looks like the following:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL\n",
    "It is possible to write SQL queries and take advantage of the Spark distributed computing capability. For doing so, it is required to create a temporary view and apply the SQL query on the corresponding view. Let us show the usage of Spark SQL using our DataFrame DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+--------------+\n",
      "|Platform|Trade Name|sum_buy_price|sum_sell_price|\n",
      "+--------+----------+-------------+--------------+\n",
      "| Plus500|    Amazon|       3528.0|        4200.0|\n",
      "| Plus500|  Facebook|        160.0|         200.0|\n",
      "|   eToro|  Facebook|        180.0|         250.0|\n",
      "| Plus500|  LinkedIn|         80.0|         140.0|\n",
      "|   eToro|   Pfeizer|         34.0|          85.5|\n",
      "| Plus500|   Bitcoin|      45000.0|       48000.0|\n",
      "| Plus500|      Sand|          5.4|           8.9|\n",
      "+--------+----------+-------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resulting_df: org.apache.spark.sql.DataFrame = [Platform: string, Trade Name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.createOrReplaceTempView(\"DF_View\")\n",
    "val resulting_df = spark.sql(\"\"\" SELECT `Platform`, \n",
    "`Trade Name`, sum(`Buy Price`) AS sum_buy_price, \n",
    "sum(`Sell Price`) AS sum_sell_price FROM DF_View \n",
    "group by Platform, `Trade Name` \"\"\")\n",
    "\n",
    "resulting_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Input and Output\n",
    "Here, I would like to bring the code for reading and writing Parquet and CSV files in Spark\n",
    "\n",
    "Various approaches for reading Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: not found: value file_path\r",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: not found: value file_path\r",
      "       val df_ = spark.read.parquet(file_path)\r",
      "                                    ^\r",
      "<console>:38: error: not found: value path_list\r",
      "       val df = spark.read.parquet(path_list:_*)\r",
      "                                   ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val df_ = spark.read.parquet(file_path)\n",
    "\n",
    "// Reading a list of paths with Parquet Files with same schemaVarious ways of reading CSV file\n",
    "\n",
    "val df = spark.read.parquet(path_list:_*)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various approaches for reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reading csv files using a map of options\n",
    "\n",
    "val df_csv_v1 = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\"))\n",
    ".csv(file_path)\n",
    "\n",
    "// Reading csv file in a single folder path\n",
    "\n",
    "val df_csv_v2 = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\"))\n",
    ".csv(folder_path)\n",
    "\n",
    "// A list of file paths with same schema\n",
    "\n",
    "val df_csv_v3 = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\"))\n",
    ".csv(file_path:_*)\n",
    "\n",
    "// An example with a single option\n",
    "\n",
    "val df_csv_v4 = spark.read.option(\"delimiter\", \",\").csv(file_path)\n",
    "\n",
    "// Using option() sequentially\n",
    "\n",
    "val df_csv_v5 = spark.read.option(\"delimiter\", \",\").option(\"inferSchema, \"true\")\n",
    ".csv(file_path)\n",
    "\n",
    "// Reading CSV file with a specific schema\n",
    "\n",
    "val df_csv_v6 = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "      .schema(schema)\n",
    "      .load(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Parquet and CSV Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// writes by the predefined number of partitions\n",
    "\n",
    "DF.spark.write.parquet(folder_path)\n",
    "\n",
    "// defining some options and number of partitions using coalesce()\n",
    "\n",
    "DF.coalesce().spark.write.mode('append').parquet(folder_path)\n",
    "\n",
    "\n",
    "// Using partition by\n",
    "\n",
    "DF.write.mode('append').partitionBy('col_name').parquet(folder_path)\n",
    "\n",
    "\n",
    "// writing CSV files\n",
    "\n",
    "DF.write.csv(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
